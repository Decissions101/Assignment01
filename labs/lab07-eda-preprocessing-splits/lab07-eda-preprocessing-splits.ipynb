{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e5b5491",
   "metadata": {},
   "source": [
    "# Lab 07: Exploratory Data Analysis, Preprocessing & Evaluation Splits\n",
    "\n",
    "**ING3513 - Introduction to Artificial Intelligence and Machine Learning**\n",
    "\n",
    "Today's lab is a practical walkthrough of how to interrogate a dataset before modeling, clean up issues, scale features, and design evaluation splits that keep you honest. You'll combine visual intuition (Seaborn) with scikit-learn utilities to spot data quality traps before they ruin your models.\n",
    "\n",
    "**What you'll learn:**\n",
    "\n",
    "- **Summary statistics** — quick ways to spot suspicious distributions\n",
    "- **Missing values & outliers** — detection, visualization, and handling strategies\n",
    "- **Normalization** — when to use StandardScaler vs MinMaxScaler, and how to denormalize for interpretation\n",
    "- **Correlation & redundancy** — identifying multicollinearity and potential leakage\n",
    "- **Evaluation splits** — train/test, train/val/test, and cross-validation protocols\n",
    "\n",
    "### Scope: What This Lab _Is_\n",
    "\n",
    "- Understanding data quality, feature distributions, and relationships\n",
    "- Practicing leakage-safe preprocessing with scikit-learn tools\n",
    "- Comparing evaluation strategies (train/test, train/val/test, cross-validation)\n",
    "\n",
    "### Scope: What This Lab _Is Not_\n",
    "\n",
    "- Training state-of-the-art models or feature engineering for accuracy competitions\n",
    "- Exhaustive hyperparameter tuning or advanced model selection\n",
    "\n",
    "### A Note on Modeling\n",
    "\n",
    "At the end of the lab (Part 8), we'll train a simple logistic regression model to predict passenger survival on the Titanic. But **prediction accuracy is not the goal** — the model serves purely to demonstrate how different evaluation splits affect reported performance and how scikit-learn pipelines prevent data leakage. Focus on the _methodology_, not the metrics.\n",
    "\n",
    "### Why This Lab Exists\n",
    "\n",
    "In Lab 06, you saw how gradient descent automatically finds optimal parameters by crawling along an error surface. But gradient descent assumes your data is clean, scaled, and split properly. **Bad data beats good models every time.**\n",
    "\n",
    "Today, you'll learn to interrogate a dataset _before_ modeling — the skills that separate practitioners who get lucky from those who get reliable results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"pandas {pd.__version__} | numpy {np.__version__} | seaborn {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cc83b2",
   "metadata": {},
   "source": [
    "## Part 0: Setup & Dataset\n",
    "\n",
    "We'll use the classic **Titanic** dataset from Seaborn. This section establishes the contract for the rest of the lab: we load the data, decide on a target (`survived`), and identify columns that could leak information or should be dropped before modeling.\n",
    "\n",
    "The Titanic dataset has real-world missingness (especially in `age` and `deck`), making it ideal for practicing missing-value detection. We also track feature roles (numerical vs. categorical, feature vs. target vs. leakage-prone) so preprocessing steps are explicit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_titanic = sns.load_dataset(\"titanic\")\n",
    "df = raw_titanic.copy()\n",
    "\n",
    "# Columns to drop before modeling (grouped by reason)\n",
    "#\n",
    "# 1. TRUE LEAKAGE — directly encodes the target\n",
    "leakage_cols = [\"alive\"]  # \"yes\"/\"no\" is just survived in string form\n",
    "\n",
    "# 2. DERIVED FEATURES — reconstructable from other columns (redundant)\n",
    "derived_cols = [\"adult_male\", \"who\", \"alone\"]\n",
    "# - adult_male: derived from sex + age (True if male >= 16)\n",
    "# - who: derived from sex + age (\"man\"/\"woman\"/\"child\")\n",
    "# - alone: derived from sibsp + parch (True if both are 0)\n",
    "\n",
    "# 3. REDUNDANT ENCODINGS — same info, different format (pick one)\n",
    "redundant_cols = [\"class\", \"embark_town\"]\n",
    "# - class: categorical version of pclass (\"First\"/\"Second\"/\"Third\") — keep pclass\n",
    "# - embark_town: full name of embarked (\"Southampton\" vs \"S\") — keep embarked\n",
    "\n",
    "# 4. HIGH MISSINGNESS — not leakage, but ~77% missing\n",
    "# deck could be informative (cabin location → class → survival), but we drop it\n",
    "# for simplicity. Alternative: keep it and add \"Unknown\" category.\n",
    "high_missing_cols = [\"deck\"]\n",
    "\n",
    "# Combine all columns to exclude from features\n",
    "drop_cols = leakage_cols + derived_cols + redundant_cols + high_missing_cols\n",
    "\n",
    "target_col = \"survived\"\n",
    "feature_cols = [col for col in df.columns if col not in drop_cols + [target_col]]\n",
    "\n",
    "numeric_cols = [col for col in feature_cols if pd.api.types.is_numeric_dtype(df[col])]\n",
    "categorical_cols = sorted(list(set(feature_cols) - set(numeric_cols)))\n",
    "\n",
    "print(f\"Dataset shape: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "print(\n",
    "    f\"Target column: {target_col} (classes: {sorted(df[target_col].dropna().unique())})\"\n",
    ")\n",
    "print(f\"Feature columns ({len(feature_cols)}): {feature_cols}\")\n",
    "print(f\"Numeric features ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"Categorical features ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print()\n",
    "print(\"Dropped columns:\")\n",
    "print(f\"  Leakage:            {leakage_cols}\")\n",
    "print(f\"  Derived:            {derived_cols}\")\n",
    "print(f\"  Redundant encoding: {redundant_cols}\")\n",
    "print(f\"  High missingness:   {high_missing_cols}\")\n",
    "\n",
    "print()\n",
    "print(\"Class distribution (0 = died, 1 = survived):\")\n",
    "display(df[target_col].value_counts(dropna=False).to_frame(\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb4f1f",
   "metadata": {},
   "source": [
    "## Part 1: First Look\n",
    "\n",
    "Quick reconnaissance helps you spot suspicious columns immediately. We'll inspect `df.head()` and a random sample to catch obvious data-entry issues, then report `df.shape` and `df.dtypes` to understand the column types.\n",
    "\n",
    "Building a \"column roles\" table (ID-like, features, target) makes it explicit which columns need special treatment — categorical text, binary flags, timestamps, or identifiers that must be dropped before modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bbf93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape: {df.shape}\")\n",
    "print()\n",
    "print(\"Peek at the first 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print()\n",
    "print(\"Random sample (5 rows)\")\n",
    "display(df.sample(5, random_state=SEED))\n",
    "\n",
    "print()\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "column_roles = []\n",
    "for col in df.columns:\n",
    "    if col == target_col:\n",
    "        role = \"target\"\n",
    "    elif col in leakage_cols:\n",
    "        role = \"leakage (drop)\"\n",
    "    elif col in derived_cols:\n",
    "        role = \"derived (drop)\"\n",
    "    elif col in redundant_cols:\n",
    "        role = \"redundant encoding (drop)\"\n",
    "    elif col in high_missing_cols:\n",
    "        role = \"high missingness (drop)\"\n",
    "    elif col in feature_cols:\n",
    "        role = \"feature\"\n",
    "    else:\n",
    "        role = \"ignored\"\n",
    "    column_roles.append(\n",
    "        {\n",
    "            \"column\": col,\n",
    "            \"role\": role,\n",
    "            \"dtype\": str(df[col].dtype),\n",
    "            \"missing_%\": round(df[col].isna().mean() * 100, 2),\n",
    "        }\n",
    "    )\n",
    "\n",
    "roles_df = pd.DataFrame(column_roles).sort_values(\"role\")\n",
    "print()\n",
    "print(\"Column role summary:\")\n",
    "display(roles_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e316e4",
   "metadata": {},
   "source": [
    "> **Prompt:** Look at the `alive` column values alongside `survived`. Why is this a textbook example of data leakage? What would happen to your test accuracy if you accidentally kept it as a feature?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c2f571",
   "metadata": {},
   "source": [
    "## Part 2: Summary Statistics\n",
    "\n",
    "Summary statistics give a quick snapshot of numeric columns. We use `df.describe().T` to see count, mean, std, min, quartiles, and max in one view.\n",
    "\n",
    "**What to look for:**\n",
    "\n",
    "- Columns with the largest standard deviation (most spread) — likely candidates for scaling.\n",
    "- Columns with tiny standard deviation — possible near-constant features that add no signal.\n",
    "- Min/max values outside believable domain ranges — signs of data-entry errors or sentinel values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48650210",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_summary = df[numeric_cols].describe().T\n",
    "print(\"Numeric summary (transpose):\")\n",
    "display(numeric_summary)\n",
    "\n",
    "largest_std = numeric_summary.sort_values(\"std\", ascending=False).head(3)[\"std\"]\n",
    "smallest_std = numeric_summary.sort_values(\"std\", ascending=True).head(3)[\"std\"]\n",
    "\n",
    "print()\n",
    "print(\"Top 3 columns by std (largest spread):\")\n",
    "display(largest_std.to_frame())\n",
    "print()\n",
    "print(\"Bottom 3 columns by std (near-constant risk):\")\n",
    "display(smallest_std.to_frame())\n",
    "\n",
    "scale_candidates = numeric_summary[numeric_summary[\"std\"] > 50].index.tolist()\n",
    "near_constant = numeric_summary[numeric_summary[\"std\"] < 5].index.tolist()\n",
    "print()\n",
    "print(f\"Likely scaling candidates (std > 50): {scale_candidates}\")\n",
    "print(f\"Potential near-constant numeric features (std < 5): {near_constant}\")\n",
    "\n",
    "# Domain rule checks\n",
    "domain_rules = {\n",
    "    \"age\": (0, 100),\n",
    "    \"fare\": (0, 600),\n",
    "    \"sibsp\": (0, 10),\n",
    "    \"parch\": (0, 10),\n",
    "    \"pclass\": (1, 3),\n",
    "}\n",
    "violations = []\n",
    "for col, (lower, upper) in domain_rules.items():\n",
    "    if col not in df.columns:\n",
    "        continue\n",
    "    mask = df[col].notna() & ((df[col] < lower) | (df[col] > upper))\n",
    "    if mask.any():\n",
    "        violations.append(\n",
    "            {\"column\": col, \"count\": int(mask.sum()), \"bounds\": (lower, upper)}\n",
    "        )\n",
    "\n",
    "if violations:\n",
    "    print()\n",
    "    print(\"Possible domain violations:\")\n",
    "    display(pd.DataFrame(violations))\n",
    "else:\n",
    "    print()\n",
    "    print(\"No domain-rule violations detected in numeric columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bcb509",
   "metadata": {},
   "source": [
    "> **Prompt:** Based on standard deviations and ranges, which columns seem most in need of scaling? Are any columns suspiciously flat or impossible?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade349b9",
   "metadata": {},
   "source": [
    "## Part 3: Data Sanity Checks\n",
    "\n",
    "Before modeling, sanity-check the data for structural issues that can silently break your pipeline.\n",
    "\n",
    "**Common checks:**\n",
    "\n",
    "- **Duplicates** — `df.duplicated().sum()` counts exact row copies.\n",
    "- **Near-constant columns** — features with very few unique values add no signal. `VarianceThreshold` from scikit-learn can flag these automatically.\n",
    "- **Impossible values** — domain rules like \"mass must be positive\" catch data-entry errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be29b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = df.duplicated().sum()\n",
    "print(f\"Duplicate rows (all columns): {duplicate_rows}\")\n",
    "\n",
    "unique_counts = df.nunique(dropna=False).sort_values()\n",
    "print()\n",
    "print(\"Unique values per column (including NaN):\")\n",
    "display(unique_counts.to_frame(\"unique_values\"))\n",
    "\n",
    "near_constant_features = [\n",
    "    col for col in feature_cols if df[col].nunique(dropna=False) <= 3\n",
    "]\n",
    "print()\n",
    "print(f\"Near-constant feature candidates (<=3 unique values): {near_constant_features}\")\n",
    "\n",
    "# VarianceThreshold for numeric columns\n",
    "filled_numeric = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "vt = VarianceThreshold(threshold=1.0)\n",
    "vt.fit(filled_numeric)\n",
    "low_variance_mask = ~vt.get_support()\n",
    "low_variance_cols = filled_numeric.columns[low_variance_mask].tolist()\n",
    "print(\n",
    "    f\"VarianceThreshold flagged columns (variance < 1.0): {low_variance_cols if low_variance_cols else 'None'}\"\n",
    ")\n",
    "\n",
    "# Impossible values check\n",
    "impossible_checks = []\n",
    "for col, (lower, upper) in domain_rules.items():\n",
    "    if col not in df.columns:\n",
    "        continue\n",
    "    mask = df[col].notna() & ((df[col] < 0) | (df[col] > upper * 1.5))\n",
    "    if mask.any():\n",
    "        impossible_checks.append({\"column\": col, \"count\": int(mask.sum())})\n",
    "\n",
    "if impossible_checks:\n",
    "    print()\n",
    "    print(\"Impossible values detected (needs manual review):\")\n",
    "    display(pd.DataFrame(impossible_checks))\n",
    "else:\n",
    "    print()\n",
    "    print(\"No impossible-value violations detected (given current simple rules).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ebbad0",
   "metadata": {},
   "source": [
    "> **Prompt:** Which issues are true data errors versus rare but plausible events? How would you confirm before deleting anything?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233ca68c",
   "metadata": {},
   "source": [
    "## Part 4: Missing Values\n",
    "\n",
    "Missing data is one of the most common data quality issues. We compute missing counts and percentages per column, then visualize the worst offenders with a bar plot.\n",
    "\n",
    "**Key questions:**\n",
    "\n",
    "- Are there sentinel values (e.g., `-999`) masquerading as real data?\n",
    "- Does missingness appear random, or is it tied to specific categories (systematic missingness)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0410da",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_stats = df.isna().sum().to_frame(\"missing_count\")\n",
    "missing_stats[\"missing_%\"] = (missing_stats[\"missing_count\"] / len(df) * 100).round(2)\n",
    "missing_stats = missing_stats.sort_values(\"missing_%\", ascending=False)\n",
    "\n",
    "print(\"Missingness summary:\")\n",
    "display(missing_stats)\n",
    "\n",
    "missing_top = missing_stats[missing_stats[\"missing_count\"] > 0].head(10).reset_index()\n",
    "if not missing_top.empty:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.barplot(data=missing_top, x=\"index\", y=\"missing_%\", color=\"#1f77b4\")\n",
    "    plt.title(\"Top Missing Columns (percentage)\")\n",
    "    plt.ylabel(\"Missing %\")\n",
    "    plt.xlabel(\"Column\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values detected - nice but rare!\")\n",
    "\n",
    "# Sentinel value check\n",
    "sentinel_values = [-999, 9999]\n",
    "sentinel_hits = {}\n",
    "for col in numeric_cols:\n",
    "    hits = df[col].isin(sentinel_values).sum()\n",
    "    if hits:\n",
    "        sentinel_hits[col] = int(hits)\n",
    "\n",
    "if sentinel_hits:\n",
    "    print(\"Potential sentinel values found:\")\n",
    "    display(pd.Series(sentinel_hits, name=\"count\"))\n",
    "else:\n",
    "    print()\n",
    "    print(\"No obvious sentinel values (like -999) were found in numeric columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff278cd1",
   "metadata": {},
   "source": [
    "> **Prompt:** Does missingness appear random or tied to specific passenger classes or embarkation points? What hypotheses would you test next?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f5085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore whether missingness is systematic (e.g., tied to passenger class or embarkation)\n",
    "missing_by_pclass = (\n",
    "    df.groupby(\"pclass\")[numeric_cols]\n",
    "    .apply(lambda g: g.isna().sum(), include_groups=False)\n",
    "    .T\n",
    ")\n",
    "print(\"Missing values by passenger class:\")\n",
    "display(missing_by_pclass)\n",
    "\n",
    "print()\n",
    "print(\"Missing values by embarkation port:\")\n",
    "missing_by_embarked = (\n",
    "    df.groupby(\"embarked\")[numeric_cols]\n",
    "    .apply(lambda g: g.isna().sum(), include_groups=False)\n",
    "    .T\n",
    ")\n",
    "display(missing_by_embarked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d40425",
   "metadata": {},
   "source": [
    "> **Prompt:** Is age missingness evenly spread across classes, or concentrated in one? If 3rd class has more missing ages, what imputation strategy might be biased?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a1067c",
   "metadata": {},
   "source": [
    "## Part 5: Outliers & Heavy Tails\n",
    "\n",
    "Outliers can be measurement errors, rare-but-real events, or signs of a heavy-tailed distribution. We'll pick a few numeric columns, plot histograms and boxplots, then count outliers using two common rules.\n",
    "\n",
    "**Two common outlier detection rules:**\n",
    "\n",
    "| Method           | Formula                                                  | Typical threshold              |\n",
    "| ---------------- | -------------------------------------------------------- | ------------------------------ |\n",
    "| **IQR rule**     | Outside $[Q_1 - 1.5 \\cdot IQR,\\; Q_3 + 1.5 \\cdot IQR]$   | 1.5x IQR (default in boxplots) |\n",
    "| **Z-score rule** | $\\lvert z \\rvert = \\lvert (x - \\mu) / \\sigma \\rvert > k$ | Typically $k = 3$              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dce05ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_cols = [\"age\", \"fare\", \"sibsp\"]\n",
    "fig, axes = plt.subplots(len(outlier_cols), 2, figsize=(12, len(outlier_cols) * 3.5))\n",
    "axes = np.array(axes).reshape(len(outlier_cols), 2)\n",
    "for idx, col in enumerate(outlier_cols):\n",
    "    sns.histplot(df[col], kde=True, ax=axes[idx, 0], color=\"#1f77b4\")\n",
    "    axes[idx, 0].set_title(f\"{col} histogram\")\n",
    "    sns.boxplot(x=df[col], ax=axes[idx, 1], color=\"#ff7f0e\")\n",
    "    axes[idx, 1].set_title(f\"{col} boxplot (IQR view)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9226390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_outlier_mask(series: pd.Series, multiplier: float = 1.5):\n",
    "    \"\"\"Return boolean mask where True = outlier by IQR rule.\"\"\"\n",
    "    clean = series.dropna()\n",
    "    if clean.empty:\n",
    "        return pd.Series(False, index=series.index)\n",
    "    q1, q3 = clean.quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - multiplier * iqr\n",
    "    upper = q3 + multiplier * iqr\n",
    "    mask = (series < lower) | (series > upper)\n",
    "    return mask.fillna(False)\n",
    "\n",
    "\n",
    "def zscore_outlier_mask(series: pd.Series, threshold: float = 3.0):\n",
    "    \"\"\"Return boolean mask where True = outlier by z-score rule.\"\"\"\n",
    "    clean = series.dropna()\n",
    "    if clean.std(ddof=0) == 0 or clean.empty:\n",
    "        return pd.Series(False, index=series.index)\n",
    "    z = (series - clean.mean()) / clean.std(ddof=0)\n",
    "    mask = z.abs() > threshold\n",
    "    return mask.fillna(False)\n",
    "\n",
    "\n",
    "outlier_summary = []\n",
    "for col in outlier_cols:\n",
    "    iqr_mask = iqr_outlier_mask(df[col])\n",
    "    z_mask = zscore_outlier_mask(df[col])\n",
    "    outlier_summary.append(\n",
    "        {\n",
    "            \"column\": col,\n",
    "            \"iqr_outliers\": int(iqr_mask.sum()),\n",
    "            \"z>3_outliers\": int(z_mask.sum()),\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"Outlier counts by method:\")\n",
    "display(pd.DataFrame(outlier_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2f0588",
   "metadata": {},
   "source": [
    "> **Prompt:** For each flagged outlier, would you treat it as an error, a rare-but-real event, or something to model explicitly?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4af7d5",
   "metadata": {},
   "source": [
    "## Part 6: Relationships (Scatter + Correlation)\n",
    "\n",
    "Visualizing relationships helps you understand which features might predict the target and which features are redundant with each other.\n",
    "\n",
    "**Understanding the family size features:**\n",
    "\n",
    "- `sibsp` = number of siblings/spouses aboard\n",
    "- `parch` = number of parents/children aboard\n",
    "\n",
    "We'll show a **survival rate heatmap** for family size — each cell shows what fraction of passengers with that (sibsp, parch) combination survived. This reveals patterns like \"medium-sized families survived better than solo travelers or very large families.\"\n",
    "\n",
    "A **correlation heatmap** summarizes all pairwise numeric relationships, and we'll also compute **feature-target correlations** to see which numeric features are most associated with survival. Watch for:\n",
    "\n",
    "- **Predictive features** — features that correlate with the target (positive or negative).\n",
    "- **Redundant features** — highly correlated pairs that carry the same information.\n",
    "- **Leakage risks** — features that correlate suspiciously well with the target (they might encode the answer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a3b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-feature scatter: Age vs Fare\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "subset = df[[\"age\", \"fare\", target_col]].dropna()\n",
    "sns.scatterplot(data=subset, x=\"age\", y=\"fare\", hue=target_col, ax=axes[0], alpha=0.85)\n",
    "axes[0].set_title(\"Age vs Fare (expect weak relationship)\")\n",
    "\n",
    "# Family size survival heatmap: shows survival RATE by sibsp/parch combination\n",
    "survival_by_family = (\n",
    "    df.groupby([\"sibsp\", \"parch\"])[target_col].mean().unstack(fill_value=0)\n",
    ")\n",
    "sns.heatmap(\n",
    "    survival_by_family,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"RdYlGn\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    ax=axes[1],\n",
    "    cbar_kws={\"label\": \"Survival rate\"},\n",
    ")\n",
    "axes[1].set_title(\"Survival rate by family size (sibsp × parch)\")\n",
    "axes[1].set_xlabel(\"parch (parents/children)\")\n",
    "axes[1].set_ylabel(\"sibsp (siblings/spouses)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature vs. target: use violin/box plots for binary targets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Violin plot shows distribution shape (better than strip plot for overlapping data)\n",
    "sns.violinplot(\n",
    "    data=df,\n",
    "    x=target_col,\n",
    "    y=\"fare\",\n",
    "    hue=target_col,\n",
    "    split=False,\n",
    "    inner=\"quartile\",\n",
    "    ax=axes[0],\n",
    "    legend=False,\n",
    ")\n",
    "axes[0].set_title(\"Fare by survival (violin plot)\")\n",
    "\n",
    "# Box plot shows distribution summary\n",
    "sns.boxplot(data=df, x=target_col, y=\"age\", hue=target_col, ax=axes[1], legend=False)\n",
    "axes[1].set_title(\"Age by survival (box plot)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12a36fe",
   "metadata": {},
   "source": [
    "> **Prompt:** In the scatter plots, do survivors (orange) cluster in different regions than non-survivors (blue)? Which feature looks more predictive of survival — `fare` or `age`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0fb7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df[numeric_cols + [target_col]].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.7},\n",
    ")\n",
    "plt.title(\"Correlation heatmap (features + target)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# List top correlated pairs (excluding self-correlation)\n",
    "abs_corr = (\n",
    "    corr_matrix.abs()\n",
    "    .where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    .stack()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "top_pairs = abs_corr.head(10).rename(\"abs_correlation\").reset_index()\n",
    "top_pairs.columns = [\"feature_a\", \"feature_b\", \"abs_correlation\"]\n",
    "print(\"Top correlated pairs (including target):\")\n",
    "display(top_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8941d043",
   "metadata": {},
   "source": [
    "> **Prompt:** Which feature pairs have |correlation| > 0.4? Would dropping one of a highly correlated pair simplify your model without losing predictive power?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b42ed7",
   "metadata": {},
   "source": [
    "## Part 7: Feature Scaling with scikit-learn\n",
    "\n",
    "Many algorithms (PCA, logistic regression, k-NN, neural networks) are sensitive to feature scales. **Feature scaling** is the umbrella term for transforming features to comparable ranges.\n",
    "\n",
    "> **Terminology note:** Strictly speaking, _normalization_ means scaling to a bounded range like [0, 1] (what `MinMaxScaler` does), while _standardization_ means centering to mean=0 and scaling to std=1 (what `StandardScaler` does). However, you'll often hear people use \"normalization\" loosely to mean any scaling — don't let the sloppy terminology confuse you.\n",
    "\n",
    "We'll compare two common approaches:\n",
    "\n",
    "- **StandardScaler** (standardization) — centers to mean=0 and scales to std=1. Preferred for PCA and gradient-based methods.\n",
    "- **MinMaxScaler** (normalization) — rescales to [0, 1]. Useful when you need bounded ranges or for k-NN with features on very different scales.\n",
    "\n",
    "| Scaler           | Transform                                  | When to use                                    |\n",
    "| ---------------- | ------------------------------------------ | ---------------------------------------------- |\n",
    "| `StandardScaler` | $z = (x - \\mu) / \\sigma$                   | PCA, logistic regression, SVM, neural networks |\n",
    "| `MinMaxScaler`   | $x' = (x - x_{min}) / (x_{max} - x_{min})$ | k-NN, when bounded [0,1] range is needed       |\n",
    "\n",
    "### Reversing the Transformation\n",
    "\n",
    "After training, you often need to convert predictions or coefficients back to the **original scale** for interpretation. Scikit-learn scalers provide `inverse_transform()` for exactly this purpose — essential when you need to report results in human-readable units (e.g., \"predicted fare: $32.50\" instead of \"predicted fare: -0.31\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae205e",
   "metadata": {},
   "source": [
    "### StandardScaler (Standardization)\n",
    "\n",
    "StandardScaler transforms each feature to have mean=0 and standard deviation=1. This is the most common choice for algorithms that assume normally distributed inputs or use gradient-based optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_imputed = df[numeric_cols].copy()\n",
    "numeric_imputed = numeric_imputed.fillna(numeric_imputed.median())\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "standard_scaled = pd.DataFrame(\n",
    "    standard_scaler.fit_transform(numeric_imputed),\n",
    "    columns=numeric_cols,\n",
    ")\n",
    "\n",
    "focus_col = \"fare\"\n",
    "comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"metric\": [\"mean_before\", \"std_before\", \"mean_after\", \"std_after\"],\n",
    "        \"value\": [\n",
    "            numeric_imputed[focus_col].mean(),\n",
    "            numeric_imputed[focus_col].std(ddof=0),\n",
    "            standard_scaled[focus_col].mean(),\n",
    "            standard_scaled[focus_col].std(ddof=0),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "print(f\"{focus_col} before/after standardization:\")\n",
    "display(comparison)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "sns.histplot(numeric_imputed[focus_col], kde=True, ax=axes[0], color=\"#1f77b4\")\n",
    "axes[0].set_title(f\"{focus_col} (original scale)\")\n",
    "sns.histplot(standard_scaled[focus_col], kde=True, ax=axes[1], color=\"#ff7f0e\")\n",
    "axes[1].set_title(f\"{focus_col} (standardized)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312bb920",
   "metadata": {},
   "source": [
    "### MinMaxScaler (Normalization)\n",
    "\n",
    "MinMaxScaler rescales features to a fixed range, typically [0, 1]. Unlike StandardScaler, it preserves zero entries in sparse data and guarantees bounded outputs — useful for algorithms sensitive to feature magnitudes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709661b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = MinMaxScaler()\n",
    "minmax_scaled = pd.DataFrame(\n",
    "    minmax_scaler.fit_transform(numeric_imputed),\n",
    "    columns=numeric_cols,\n",
    ")\n",
    "\n",
    "focus_col_minmax = \"age\"\n",
    "minmax_comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"metric\": [\"min_before\", \"max_before\", \"min_after\", \"max_after\"],\n",
    "        \"value\": [\n",
    "            numeric_imputed[focus_col_minmax].min(),\n",
    "            numeric_imputed[focus_col_minmax].max(),\n",
    "            minmax_scaled[focus_col_minmax].min(),\n",
    "            minmax_scaled[focus_col_minmax].max(),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "print(f\"{focus_col_minmax} before/after MinMax scaling:\")\n",
    "display(minmax_comparison)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "sns.histplot(numeric_imputed[focus_col_minmax], kde=True, ax=axes[0], color=\"#2ca02c\")\n",
    "axes[0].set_title(f\"{focus_col_minmax} (original scale)\")\n",
    "sns.histplot(minmax_scaled[focus_col_minmax], kde=True, ax=axes[1], color=\"#d62728\")\n",
    "axes[1].set_title(f\"{focus_col_minmax} (MinMax scaled)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f836c60",
   "metadata": {},
   "source": [
    "### Inverse Transform: Converting Back to Original Scale\n",
    "\n",
    "After modeling, you'll often need to **inverse transform** values back to interpretable units. Scikit-learn scalers store the parameters learned during `fit()` and expose `inverse_transform()` to reverse the scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate denormalization (inverse_transform)\n",
    "# Suppose we have a normalized prediction and need to report it in original units\n",
    "\n",
    "# Pick a sample of normalized fare values\n",
    "sample_normalized = standard_scaled[[\"fare\"]].head(5)\n",
    "\n",
    "# Denormalize back to original scale\n",
    "# Note: inverse_transform expects all columns that were fit, so we use the full array\n",
    "sample_denormalized = standard_scaler.inverse_transform(standard_scaled.head(5))\n",
    "sample_denormalized_df = pd.DataFrame(sample_denormalized, columns=numeric_cols)\n",
    "\n",
    "# Compare: normalized vs denormalized vs actual original\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"original\": numeric_imputed[\"fare\"].head(5).values,\n",
    "        \"normalized\": standard_scaled[\"fare\"].head(5).values,\n",
    "        \"denormalized\": sample_denormalized_df[\"fare\"].values,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Round-trip demonstration: original → normalized → denormalized\")\n",
    "display(comparison_df)\n",
    "\n",
    "print()\n",
    "print(\"✓ Denormalized values match the original (within floating-point precision)\")\n",
    "print(\"  This is essential for interpreting model outputs in real-world units.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e2dd2f",
   "metadata": {},
   "source": [
    "> **Prompt:** Which scaler is more appropriate for PCA or k-NN on this dataset, and why? When might you deliberately keep features on their original scale? In what scenarios would you need to denormalize your model's predictions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2190a335",
   "metadata": {},
   "source": [
    "## Part 8: Split Strategies & Leakage Safety\n",
    "\n",
    "We'll use a lightweight baseline model (logistic regression) purely to demonstrate how evaluation protocols change the story. Every pipeline below keeps preprocessing inside `Pipeline`/`ColumnTransformer` so scalers learn from training folds only - **this prevents data leakage automatically**.\n",
    "\n",
    "### A Quick Word on Logistic Regression\n",
    "\n",
    "**Logistic regression** is a classification algorithm that predicts the probability of a binary outcome (here: survived vs. died). Despite the name, it's used for _classification_, not regression.\n",
    "\n",
    "- **How it works:** Computes a weighted sum of features, then squashes the result through a sigmoid function to get a probability between 0 and 1.\n",
    "- **Decision rule:** If probability > 0.5, predict class 1 (survived); otherwise predict class 0 (died).\n",
    "- **Why we use it here:** It's simple, fast, and interpretable — perfect for demonstrating evaluation strategies without getting distracted by model complexity.\n",
    "\n",
    "You'll explore logistic regression in more depth in later labs. For now, just think of it as a \"good enough\" classifier that lets us focus on _how we evaluate_, not _what we evaluate_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae7196",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = df.dropna(subset=[target_col]).copy()\n",
    "X_all = model_df[feature_cols]\n",
    "y_all = model_df[target_col]\n",
    "\n",
    "numeric_features = [col for col in numeric_cols if col in feature_cols]\n",
    "categorical_features = [col for col in categorical_cols if col in feature_cols]\n",
    "\n",
    "print(f\"Modeling rows: {X_all.shape[0]} (dropped rows missing target)\")\n",
    "print(f\"Numeric features used: {numeric_features}\")\n",
    "print(f\"Categorical features used: {categorical_features}\")\n",
    "\n",
    "\n",
    "def make_preprocessor():\n",
    "    \"\"\"Build a ColumnTransformer for numeric + categorical features.\"\"\"\n",
    "    # Numeric pipeline: fill missing values with median, then standardize to mean=0, std=1\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"imputer\",\n",
    "                SimpleImputer(strategy=\"median\"),\n",
    "            ),  # Replace NaN with column median\n",
    "            (\"scaler\", StandardScaler()),  # Center and scale: z = (x - mean) / std\n",
    "        ]\n",
    "    )\n",
    "    # Categorical pipeline: fill missing with most common value, then one-hot encode\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"imputer\",\n",
    "                SimpleImputer(strategy=\"most_frequent\"),\n",
    "            ),  # Replace NaN with mode\n",
    "            (\n",
    "                \"encoder\",\n",
    "                OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "            ),  # Create binary columns per category\n",
    "        ]\n",
    "    )\n",
    "    # ColumnTransformer applies different pipelines to different column subsets\n",
    "    # This keeps numeric and categorical preprocessing separate and explicit\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def build_baseline_pipeline(C: float = 1.0):\n",
    "    \"\"\"Return a full pipeline: preprocessing -> logistic regression.\n",
    "\n",
    "    C controls regularization strength (smaller C = stronger regularization).\n",
    "    \"\"\"\n",
    "    return Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocess\", make_preprocessor()),  # All preprocessing in one step\n",
    "            (\"model\", LogisticRegression(max_iter=500, C=C)),  # Classification model\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4600ee",
   "metadata": {},
   "source": [
    "### 8.0 Baseline Model (model_0)\n",
    "\n",
    "Before building real models, we establish a **baseline** — the minimum accuracy we should beat. A `DummyClassifier` with `strategy=\"most_frequent\"` always predicts the majority class. If our logistic regression can't beat this, we have no real signal.\n",
    "\n",
    "**Important:** We split off a single test set here that will be reused in sections 8.1–8.3. This lets us directly compare results across different training strategies on the exact same held-out data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split off test set ONCE — this same test set is reused in 8.1, 8.2, and 8.3\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, stratify=y_all, random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Full dataset: {len(X_all)} rows\")\n",
    "print(f\"Train+Val pool: {len(X_trainval)} | Test (held out): {len(X_test)}\")\n",
    "print()\n",
    "\n",
    "# model_0: always predict the majority class\n",
    "model_0 = DummyClassifier(strategy=\"most_frequent\")\n",
    "model_0.fit(X_trainval, y_trainval)\n",
    "baseline_acc = model_0.score(X_test, y_test)\n",
    "\n",
    "print(\"\\n→ Any useful model must beat this floor!\")\n",
    "\n",
    "print(f\"Target distribution: {y_all.value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"\\nmodel_0 (majority class): test accuracy = {baseline_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d677c2ab",
   "metadata": {},
   "source": [
    "### 8.1 Train/Test Split (baseline sanity check)\n",
    "\n",
    "Use when you need a quick read on signal and are not tuning hyperparameters. The pipeline automatically fits the scaler on train data only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd006c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the full train+val pool as training data (no validation split)\n",
    "print(f\"Train size: {len(X_trainval)} | Test size: {len(X_test)}\")\n",
    "print()\n",
    "\n",
    "tt_pipeline = build_baseline_pipeline(C=1.0)\n",
    "tt_pipeline.fit(X_trainval, y_trainval)\n",
    "train_acc = tt_pipeline.score(X_trainval, y_trainval)\n",
    "test_acc = tt_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"Train accuracy: {train_acc:.3f}\")\n",
    "print(f\"Test accuracy : {test_acc:.3f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "tt_preds = tt_pipeline.predict(X_test)\n",
    "print(\"Classification report (test set):\")\n",
    "print(classification_report(y_test, tt_preds, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eecb4e",
   "metadata": {},
   "source": [
    "> **Prompt:** Is the gap between train and test accuracy large? What would a big gap suggest about overfitting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed982d",
   "metadata": {},
   "source": [
    "### 8.2 Train/Val/Test Split (model selection without CV)\n",
    "\n",
    "Use when you expect to iterate on preprocessing choices or **hyperparameters**, and you want a final untouched test set for the report.\n",
    "\n",
    "**What are hyperparameters?** Unlike model _parameters_ (e.g., regression coefficients) that are learned from data, **hyperparameters** are settings you choose _before_ training — they control how the learning algorithm behaves. Examples include:\n",
    "\n",
    "- Regularization strength (`C` in logistic regression)\n",
    "- Learning rate (in gradient descent)\n",
    "- Number of trees (in random forests)\n",
    "\n",
    "You can't learn hyperparameters from training data without risking overfitting — that's why you need a separate **validation set** to compare different choices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecae784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train+val pool into train and validation (test set unchanged from 8.0)\n",
    "X_train_tv, X_val, y_train_tv, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.2, stratify=y_trainval, random_state=SEED\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Train: {len(X_train_tv)} | Val: {len(X_val)} | Test: {len(X_test)} (same as 8.0)\"\n",
    ")\n",
    "print()\n",
    "\n",
    "# Try different regularization strengths (log scale is typical for C)\n",
    "Cs = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "val_results = []\n",
    "for C in Cs:\n",
    "    candidate = build_baseline_pipeline(C=C)\n",
    "    candidate.fit(X_train_tv, y_train_tv)\n",
    "    val_acc = candidate.score(X_val, y_val)\n",
    "    val_results.append({\"C\": C, \"val_accuracy\": val_acc})\n",
    "\n",
    "val_df = pd.DataFrame(val_results).sort_values(\"val_accuracy\", ascending=False)\n",
    "print(\"Validation results (different regularization strengths):\")\n",
    "display(val_df)\n",
    "\n",
    "best_C = val_df.iloc[0][\"C\"]\n",
    "print()\n",
    "print(f\"Best C based on validation: {best_C}\")\n",
    "\n",
    "# Retrain on full train+val pool with best hyperparameter, then evaluate on test\n",
    "final_pipeline = build_baseline_pipeline(C=best_C)\n",
    "final_pipeline.fit(X_trainval, y_trainval)\n",
    "final_test_acc = final_pipeline.score(X_test, y_test)\n",
    "\n",
    "print()\n",
    "print(\n",
    "    f\"Final test accuracy (after selecting hyperparams on val set): {final_test_acc:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda980a",
   "metadata": {},
   "source": [
    "> **Prompt:** Did the best C from validation also perform well on test? If validation and test accuracy differ substantially, what might explain it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7304ff93",
   "metadata": {},
   "source": [
    "### 8.3 Cross-Validation (reducing variance)\n",
    "\n",
    "Use when data is small/medium and a single split is too noisy. Pipelines ensure every fold fits preprocessing on its training fold only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2babc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do 5-fold CV on the train+val pool (test set unchanged from 8.0)\n",
    "cv_pipeline = build_baseline_pipeline(C=10.0)\n",
    "cv_scores = cross_val_score(\n",
    "    cv_pipeline, X_trainval, y_trainval, cv=5, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(f\"Train+Val pool: {len(X_trainval)} | Test: {len(X_test)} (same as 8.0)\")\n",
    "print()\n",
    "print(\"Cross-validation accuracy (5-fold on train+val pool):\")\n",
    "print(cv_scores)\n",
    "print(f\"Mean ± std: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "\n",
    "# Final evaluation on the same held-out test set\n",
    "cv_pipeline.fit(X_trainval, y_trainval)\n",
    "test_acc_cv = cv_pipeline.score(X_test, y_test)\n",
    "print()\n",
    "print(f\"Final test accuracy (same test set as 8.0–8.2): {test_acc_cv:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acb44c2",
   "metadata": {},
   "source": [
    "### 8.4 When to Use What\n",
    "\n",
    "| Strategy             | When to use                                                                                                        |\n",
    "| -------------------- | ------------------------------------------------------------------------------------------------------------------ |\n",
    "| **Train/Test only**  | Quick sanity checks, no tuning, plenty of data, low stakes                                                         |\n",
    "| **Train/Val/Test**   | You will iterate on preprocessing/features/hyperparameters and need a final untouched exam set                     |\n",
    "| **Cross-Validation** | Data is limited or noisy, you want a robust estimate; often paired with a final hold-out test for the final report |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd709a89",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Explore before you model.** Summary statistics, histograms, and scatter plots reveal data quality issues that break models.\n",
    "\n",
    "2. **Missing values and outliers require judgment.** Detection is mechanical; deciding what to do about them is domain knowledge.\n",
    "\n",
    "3. **Normalization matters for distance-based methods.** Use `StandardScaler` for PCA/logistic regression; `MinMaxScaler` when you need bounded ranges. Remember to denormalize predictions when reporting results in real-world units.\n",
    "\n",
    "4. **Pipelines prevent leakage.** Fit preprocessing on train data only - scikit-learn pipelines handle this automatically inside CV.\n",
    "\n",
    "5. **Choose your split strategy deliberately.** Train/test for quick checks; train/val/test for iterative tuning; CV for robust estimates on small data.\n",
    "\n",
    "### The Red Thread from Lab 06\n",
    "\n",
    "In Lab 06, you saw gradient descent crawling along an error surface to find optimal weights. That algorithm _assumes_ your features are:\n",
    "\n",
    "- **Clean** — no missing values crashing the computation\n",
    "- **Normalized** — otherwise the error surface is elongated and descent zigzags\n",
    "- **Properly split** — so the test score reflects real performance, not overfitting\n",
    "\n",
    "Today you learned the preprocessing skills that make gradient descent work in practice. **Bad data beats good algorithms every time - now you know how to avoid that trap.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ing3513",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
