{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7630800",
   "metadata": {},
   "source": [
    "# Assignment 01\n",
    "\n",
    "This assignment consists of three tasks with subtasks. Every subtask has a point value and lists expectations for answers.\n",
    "\n",
    "### Overview\n",
    "\n",
    "| Task                                         | Topic                                                   | Points  |\n",
    "| -------------------------------------------- | ------------------------------------------------------- | ------- |\n",
    "| **1: PCA**                                   | Visualizing and interpreting principal components       | 20      |\n",
    "| 1.1                                          | 3D scatter plot                                         | 5       |\n",
    "| 1.2                                          | 2D scatter plot                                         | 5       |\n",
    "| 1.3                                          | Interpreting variance                                   | 5       |\n",
    "| 1.4                                          | Variance and geometry                                   | 5       |\n",
    "| **2: PCA vs t-SNE**                          | Comparing linear and nonlinear dimensionality reduction | 20      |\n",
    "| 2.1                                          | PCA on digits                                           | 2.5     |\n",
    "| 2.2                                          | t-SNE on digits                                         | 2.5     |\n",
    "| 2.3                                          | PCA vs t-SNE separation                                 | 5       |\n",
    "| 2.4                                          | Real-world application                                  | 5       |\n",
    "| 2.5                                          | t-SNE for dimensionality reduction                      | 5       |\n",
    "| **3: Breast Cancer Classification Pipeline** | End-to-end ML workflow on breast cancer data            | 60      |\n",
    "| 3.1                                          | Exploratory Data Analysis                               | 25      |\n",
    "| 3.2                                          | Train/Test Split                                        | 5       |\n",
    "| 3.3                                          | Baseline Model                                          | 5       |\n",
    "| 3.4                                          | Kitchen Sink Model                                      | 5       |\n",
    "| 3.5                                          | Build Your Own Pipeline                                 | 20      |\n",
    "|                                              | **Total**                                               | **100** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a799d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155834c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.datasets import load_breast_cancer, load_digits\n",
    "\n",
    "\n",
    "from assignment_utils import generate_annulus_4d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3848b85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: PCA\n",
    "\n",
    "You've been given a mysterious dataset with **4 dimensions** (F1, F2, F3, F4). We can't directly visualize 4D data, but we can look at 3 dimensions at a time and use color for the 4th.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4d, radius = generate_annulus_4d()\n",
    "df = pd.DataFrame(data_4d, columns=[\"F1\", \"F2\", \"F3\", \"F4\"])\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Let's visualize the first 3 dimensions (F1, F2, F3) in 3D\n",
    "# The 4th dimension (F4) is represented as color\n",
    "fig = px.scatter_3d(\n",
    "    df,\n",
    "    x=\"F1\",\n",
    "    y=\"F2\",\n",
    "    z=\"F3\",\n",
    "    color=df[\"F4\"],\n",
    "    color_continuous_scale=\"viridis\",\n",
    "    title=\"3D View of the 4D Dataset (F1, F2, F3, color=F4)\",\n",
    "    labels={\"color\": \"F4\"},\n",
    ")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.update_layout(width=800, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472977bf",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "**PCA** is a technique that finds new axes (called _principal components_) that capture the most variance in the data.\n",
    "\n",
    "**Mathematical formulation:**\n",
    "\n",
    "1. **Standardize** the data: $\\mathbf{Z} = \\frac{\\mathbf{X} - \\boldsymbol{\\mu}}{\\boldsymbol{\\sigma}}$\n",
    "\n",
    "2. Compute the **covariance matrix**: $\\mathbf{C} = \\frac{1}{n-1} \\mathbf{Z}^T \\mathbf{Z}$\n",
    "\n",
    "3. Find the **eigenvectors** and **eigenvalues** of $\\mathbf{C}$:\n",
    "   $$\\mathbf{C} \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i$$\n",
    "4. **Project** the data onto the principal components: $\\mathbf{Z}_{PC} = \\mathbf{Z} \\mathbf{V}$\n",
    "\n",
    "where $\\mathbf{V} = [\\mathbf{v}_1, \\mathbf{v}_2, \\ldots]$ are the eigenvectors sorted by decreasing eigenvalue $\\lambda_i$.\n",
    "\n",
    "**Key ideas:**\n",
    "\n",
    "- **PC1** points in the direction of maximum variance (largest $\\lambda$)\n",
    "- **PC2** is perpendicular to PC1 and captures the next most variance\n",
    "- The **explained variance ratio** for each PC is: $\\frac{\\lambda_i}{\\sum_j \\lambda_j}$\n",
    "\n",
    "If the data lies on a lower-dimensional surface, PCA can reveal it by finding the directions that matter most.\n",
    "\n",
    "Let's standardize the data first (so all features have equal scale), then apply PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4a7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize and apply PCA\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df)\n",
    "\n",
    "pca = PCA()\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Create a DataFrame with principal components\n",
    "df_pca = pd.DataFrame(data_pca, columns=[\"PC1\", \"PC2\", \"PC3\", \"PC4\"])\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Cumulative explained variance ratio:\", np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "# Get the PC directions (loadings) - each row is a PC, each column is a feature\n",
    "components = pca.components_  # Shape: (4, 4) - 4 PCs x 4 features\n",
    "\n",
    "# Create the scatter plot of scaled data (first 3 features)\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the data points\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=data_scaled[:, 0],\n",
    "        y=data_scaled[:, 1],\n",
    "        z=data_scaled[:, 2],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=2, opacity=0.5),\n",
    "        name=\"Data\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add arrows for PC1, PC2, PC3 directions\n",
    "# Arrow length proportional to explained variance ratio (with minimum for visibility)\n",
    "colors = [\"red\", \"green\", \"blue\"]\n",
    "base_scale = 5  # Base scale factor\n",
    "min_scale = 0.5  # Minimum scale so small PCs are still visible\n",
    "\n",
    "for i in range(3):\n",
    "    pc_direction = components[i, :3]  # First 3 components of each PC\n",
    "    # Scale by explained variance ratio - longer arrow = more variance\n",
    "    # Use minimum scale so all arrows are visible\n",
    "    variance_ratio = pca.explained_variance_ratio_[i] / max(\n",
    "        pca.explained_variance_ratio_\n",
    "    )\n",
    "    scale = max(base_scale * variance_ratio, min_scale)\n",
    "\n",
    "    # Arrow line\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=[0, pc_direction[0] * scale],\n",
    "            y=[0, pc_direction[1] * scale],\n",
    "            z=[0, pc_direction[2] * scale],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=colors[i], width=8),\n",
    "            name=f\"PC{i + 1} ({pca.explained_variance_ratio_[i]:.1%} var)\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Arrow head (cone)\n",
    "    fig.add_trace(\n",
    "        go.Cone(\n",
    "            x=[pc_direction[0] * scale],\n",
    "            y=[pc_direction[1] * scale],\n",
    "            z=[pc_direction[2] * scale],\n",
    "            u=[pc_direction[0]],\n",
    "            v=[pc_direction[1]],\n",
    "            w=[pc_direction[2]],\n",
    "            colorscale=[[0, colors[i]], [1, colors[i]]],\n",
    "            showscale=False,\n",
    "            sizemode=\"absolute\",\n",
    "            sizeref=0.3,\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Principal Component Directions (arrow length ∝ variance explained)\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"F1 (scaled)\",\n",
    "        yaxis_title=\"F2 (scaled)\",\n",
    "        zaxis_title=\"F3 (scaled)\",\n",
    "    ),\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8317af",
   "metadata": {},
   "source": [
    "### Task 1.1 3D scatter plot\n",
    "\n",
    "- Task: Create a 3D scatter plot using PC1, PC2, PC3 as axes\n",
    "- Points: 5\n",
    "- Expectations: A working 3D scatter plot of the PCA-transformed data (similar in style to the first 3D plot). No further analysis or comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f9718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fce4b7",
   "metadata": {},
   "source": [
    "### Task 1.2 2D scatter plot\n",
    "\n",
    "- Task: Create a 2D scatter plot using PC1, PC2 as axes\n",
    "- Points: 5\n",
    "- Expectations: A working 2D scatter plot of the PCA-transformed data. No further analysis or comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689673ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5cd7d",
   "metadata": {},
   "source": [
    "### Task 1.3 Interpreting variance\n",
    "\n",
    "- Task: How much variance do PC1 and PC2 capture together? Based on this, what can you conclude about the original 4D dataset?\n",
    "- Points: 5\n",
    "- Expectations: A written response (1-2 paragraphs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e308ce",
   "metadata": {},
   "source": [
    "#### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872f0819",
   "metadata": {},
   "source": [
    "### Task 1.4 Variance and geometry\n",
    "\n",
    "- Task: If PC1 explained 90% of the variance and PC2 only 10%, what shape would you expect the data to form? Now compare this to your actual ~50/50 split — what does this tell you about the geometry of your data?\n",
    "- Points: 5\n",
    "- Expectations: A written response (1-2 paragraphs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968012e8",
   "metadata": {},
   "source": [
    "#### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4593b0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: PCA vs t-SNE on Handwritten Digits\n",
    "\n",
    "PCA is a **linear** method — it finds directions of maximum variance using a fixed matrix multiplication. But what if the data has **nonlinear structure** that can't be captured by straight lines?\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "We'll explore this using the **UCI Optical Recognition of Handwritten Digits** dataset: 1,797 samples of 8×8 pixel grayscale images (64 dimensions).\n",
    "\n",
    "**Sources:**\n",
    "\n",
    "- [sklearn.datasets.load_digits](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html)\n",
    "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits)\n",
    "\n",
    "### Why handwritten digits have nonlinear structure\n",
    "\n",
    "Consider the digit \"3\" written in different styles: upright, slanted, thick strokes, thin strokes, rounded, angular. Each variation changes many pixels simultaneously, but not in a simple linear way. The relationship between \"slant angle\" and pixel values involves trigonometry, not just scaling.\n",
    "\n",
    "This means the collection of all \"3\"s doesn't form a blob or ellipse in 64D space — it forms a **curved manifold**. The same is true for each digit class. PCA can only project onto flat planes, so when it tries to separate these curved clusters, they overlap and blur together.\n",
    "\n",
    "### t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "**t-SNE** is a **nonlinear** dimensionality reduction technique designed for visualization. Unlike PCA, it focuses on preserving **local neighborhood structure** rather than global variance.\n",
    "\n",
    "**Why is t-SNE nonlinear?**\n",
    "\n",
    "- **PCA**: Projects data using a fixed matrix: $\\mathbf{Z} = \\mathbf{X} \\cdot \\mathbf{W}$. The same linear transformation applies to every point.\n",
    "- **t-SNE**: Uses iterative optimization that can bend and stretch different regions differently. There's no single formula — the algorithm learns a mapping that keeps neighbors together, even if that requires warping the geometry.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. **Compute pairwise similarities** in high-dimensional space: For each point, calculate the probability that it would pick each other point as a neighbor (using a Gaussian distribution)\n",
    "\n",
    "2. **Initialize points randomly** in low-dimensional space (typically 2D or 3D)\n",
    "\n",
    "3. **Optimize**: Iteratively move points to match the neighborhood structure from the original space (minimizing KL divergence)\n",
    "\n",
    "The result: points that were neighbors in 64D remain neighbors in the output space.\n",
    "\n",
    "**Key parameters:**\n",
    "\n",
    "- `n_components`: Output dimensionality (usually 2 for visualization)\n",
    "- `perplexity`: Controls the effective number of neighbors considered (typically 5–50)\n",
    "- `random_state`: Set for reproducibility, since t-SNE uses random initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e8ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load UCI Handwritten Digits dataset (8x8 images)\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "print(f\"Dataset shape: {X_digits.shape}\")\n",
    "print(f\"Each sample is an 8x8 image flattened to {X_digits.shape[1]} features\")\n",
    "\n",
    "# Show a few example digits\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap=\"gray\")\n",
    "    ax.set_title(f\"Label: {y_digits[i]}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Sample Handwritten Digits (8×8)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9598f38b",
   "metadata": {},
   "source": [
    "### Task 2.1 PCA on Digits\n",
    "\n",
    "- Task: Apply PCA to reduce the digits data from 64D to 2D and visualize the result.\n",
    "- Points: 2.5\n",
    "- Expectations: Complete the TODO lines to reduce the data to 2D using PCA. The plotting code is provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025517fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: pca_digits = ...\n",
    "# TODO: X_pca = ...\n",
    "\n",
    "# Plot PCA result\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    X_pca[:, 0], X_pca[:, 1], c=y_digits, cmap=\"tab10\", s=5, alpha=0.7\n",
    ")\n",
    "plt.colorbar(scatter, label=\"Digit\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA on Handwritten Digits\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705f66d",
   "metadata": {},
   "source": [
    "### Task 2.2 t-SNE on Digits\n",
    "\n",
    "- Task: Apply t-SNE to reduce the digits data to 2D and visualize the result.\n",
    "- Points: 2.5\n",
    "- Expectations: Complete the TODO lines to reduce the data to 2D using t-SNE. The plotting code is provided. Note: t-SNE can be slow (~30 seconds).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c243ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tsne = ... (Hint: Experiment with perplexity value)\n",
    "# TODO: X_tsne = ...\n",
    "\n",
    "# Plot t-SNE result\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    X_tsne[:, 0], X_tsne[:, 1], c=y_digits, cmap=\"tab10\", s=5, alpha=0.7\n",
    ")\n",
    "plt.colorbar(scatter, label=\"Digit\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.title(\"t-SNE on Handwritten Digits\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f12691e",
   "metadata": {},
   "source": [
    "### Task 2.3 PCA vs t-SNE separation\n",
    "\n",
    "- Task: In your t-SNE plot, you should see distinct clusters. Why doesn't PCA produce the same clear separation, even though both methods reduce to 2D?\n",
    "- Points: 5\n",
    "- Expectations: A written response (1-2 paragraphs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6256d1eb",
   "metadata": {},
   "source": [
    "#### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92190805",
   "metadata": {},
   "source": [
    "### Task 2.4 Real-world application\n",
    "\n",
    "- Task: Give one example of a dataset from your field (signals, IT, cyber, etc.) where t-SNE would be useful for exploration. What patterns or clusters might you hope to discover?\n",
    "- Points: 5\n",
    "- Expectations: A written response (2-4 paragraphs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7e499",
   "metadata": {},
   "source": [
    "#### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59519d98",
   "metadata": {},
   "source": [
    "### Task 2.5 t-SNE for dimensionality reduction\n",
    "\n",
    "- Task: With PCA, we reduced 64D digits to 2D and could easily apply the same transformation to new data. Could you do the same with t-SNE — use it as a preprocessing step for a classifier? What fundamental problem would you encounter when trying to classify new, unseen data?\n",
    "- Points: 5\n",
    "- Expectations: A written response (2-3 paragraphs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0405a18",
   "metadata": {},
   "source": [
    "#### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af0e1ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 3: Breast Cancer Classification Pipeline\n",
    "\n",
    "Now let's apply what you've learned to a real-world dataset: the **Wisconsin Breast Cancer** dataset. This dataset contains measurements from cell nuclei in breast tissue samples, and the goal is to classify tumors as **malignant** or **benign**.\n",
    "\n",
    "**Sources:**\n",
    "\n",
    "- [sklearn.datasets.load_breast_cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html)\n",
    "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)\n",
    "\n",
    "We'll work through a complete machine learning workflow:\n",
    "\n",
    "1. Exploratory Data Analysis (EDA)\n",
    "2. Train/test split\n",
    "3. Baseline model\n",
    "4. \"Kitchen sink\" model (all features, no preprocessing)\n",
    "5. Build your own pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18989855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "df_cancer = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df_cancer[\"target\"] = cancer.target\n",
    "\n",
    "print(f\"Dataset shape: {df_cancer.shape}\")\n",
    "print(f\"Target classes: {cancer.target_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bc8c8c",
   "metadata": {},
   "source": [
    "### Task 3.1 Exploratory Data Analysis (EDA)\n",
    "\n",
    "- Task: Conduct an EDA of the breast cancer dataset. For each analysis you perform, explain _why_ you chose to look at it and what it tells you.\n",
    "- Points: 25\n",
    "- Expectations: A mix of code, plots, and written commentary. Quality of reasoning and plots matters more than quantity of plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16716954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your EDA here. Add as many code and markdown cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd0ec0",
   "metadata": {},
   "source": [
    "### Task 3.2 Train/Test Split\n",
    "\n",
    "- Task: Split the data into training and test sets (80/20) before any modeling.\n",
    "- Points: 5\n",
    "- Expectations: Complete the TODO line to create an 80/20 split with `random_state=42` for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c1773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_cancer.drop(\"target\", axis=1)\n",
    "y = df_cancer[\"target\"]\n",
    "\n",
    "# TODO: Split into train/test sets\n",
    "# X_train, X_test, y_train, y_test =\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e8002",
   "metadata": {},
   "source": [
    "### Task 3.3 Baseline Model\n",
    "\n",
    "Before building a real model, it's wise to establish a **baseline** — a classifier that any real model should beat.\n",
    "\n",
    "A **confusion matrix** shows how predictions compare to actual labels:\n",
    "\n",
    "|                     | Predicted Negative  | Predicted Positive  |\n",
    "| ------------------- | ------------------- | ------------------- |\n",
    "| **Actual Negative** | TN (True Negative)  | FP (False Positive) |\n",
    "| **Actual Positive** | FN (False Negative) | TP (True Positive)  |\n",
    "\n",
    "For cancer diagnosis: FN means missing a malignant tumor (bad!), FP means a false alarm (less bad, but still costly).\n",
    "\n",
    "- Task: Run the code below, note the accuracy and examine the confusion matrix. Describe what this classifier does. Would you trust it for diagnosis? Why or why not?\n",
    "- Points: 5\n",
    "- Expectations: A written response (1-2 paragraphs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1308362",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "baseline_accuracy = dummy.score(X_test, y_test)\n",
    "print(f\"Baseline accuracy: {baseline_accuracy:.3f}\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    dummy, X_test, y_test, display_labels=cancer.target_names, cmap=\"Blues\"\n",
    ")\n",
    "plt.title(\"Confusion Matrix: Baseline Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556423c",
   "metadata": {},
   "source": [
    "#### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b26210",
   "metadata": {},
   "source": [
    "### Task 3.4 Kitchen Sink Model\n",
    "\n",
    "The \"kitchen sink\" approach: throw all features into the model without any preprocessing. Let's see what happens.\n",
    "\n",
    "**Logistic Regression** is a linear classifier that predicts the probability of a binary outcome. It models:\n",
    "\n",
    "$$P(y=1 | \\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b) = \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + b)}}$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function, $\\mathbf{w}$ are the feature weights, and $b$ is the bias. The model is trained by minimizing the logistic loss using an iterative optimizer. Here we use **SAGA** (`solver=\"saga\"`), a stochastic gradient method whose fast convergence is only guaranteed on features with approximately the same scale ([sklearn docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)).\n",
    "\n",
    "- Task: Run the code below. Did the model converge? Why or why not? Explain based on your EDA findings and how gradient-based optimization works.\n",
    "- Points: 5\n",
    "- Expectations: A written response (1-2 paragraphs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5757c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_kitchen = LogisticRegression(solver=\"saga\", max_iter=100, random_state=42)\n",
    "lr_kitchen.fit(X_train, y_train)\n",
    "\n",
    "kitchen_train_accuracy = lr_kitchen.score(X_train, y_train)\n",
    "kitchen_accuracy = lr_kitchen.score(X_test, y_test)\n",
    "print(f\"Kitchen sink train accuracy: {kitchen_train_accuracy:.3f}\")\n",
    "print(f\"Kitchen sink test accuracy:  {kitchen_accuracy:.3f}\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    lr_kitchen, X_test, y_test, display_labels=cancer.target_names, cmap=\"Blues\"\n",
    ")\n",
    "plt.title(\"Confusion Matrix: Kitchen Sink Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f415a",
   "metadata": {},
   "source": [
    "#### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f999713",
   "metadata": {},
   "source": [
    "### Task 3.5 Build Your Own Pipeline\n",
    "\n",
    "Now it's your turn. Based on your EDA findings, build a classification pipeline.\n",
    "\n",
    "A **Pipeline** chains multiple preprocessing steps and a final estimator into a single object. This ensures:\n",
    "\n",
    "- No data leakage (preprocessing is fit only on training data)\n",
    "- Clean, reproducible code\n",
    "- Easy experimentation with different configurations\n",
    "\n",
    "Example pipeline structure:\n",
    "\n",
    "```python\n",
    "Pipeline([\n",
    "    (\"step1_name\", SomeTransformer()),\n",
    "    (\"step2_name\", AnotherTransformer()),\n",
    "    (\"classifier\", SomeClassifier()),\n",
    "])\n",
    "```\n",
    "\n",
    "- Task: Build a pipeline that preprocesses the data and fits a classifier. Evaluate your model, compare it to the kitchen sink model, and justify your preprocessing choices based on your EDA insights.\n",
    "- Points: 20\n",
    "- Notes:\n",
    "  - You are free to use any preprocessing technique (e.g., StandardScaler, PCA, column selection via ColumnTransformer, or others)\n",
    "  - There is no single \"correct\" answer — the goal is thoughtful justification\n",
    "- Expectations:\n",
    "  - A working pipeline with at least one preprocessing step\n",
    "  - A confusion matrix plot for your model\n",
    "  - A comparison with the kitchen sink model's confusion matrix\n",
    "  - A reflection on your model's errors — consider which types of mistakes matter most in a medical diagnosis context\n",
    "  - A brief explanation (1-2 paragraphs) of why you chose your preprocessing steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6405ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build your pipeline\n",
    "# Consider: What preprocessing steps would help based on your EDA?\n",
    "# Available transformers: StandardScaler, PCA, ColumnTransformer, etc.\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        # Example: ColumnTransformer to select/transform specific columns\n",
    "        # (\"preprocessor\", ColumnTransformer([\n",
    "        #     (\"selected_features\", StandardScaler(), [\"mean radius\", \"mean texture\", ...]),\n",
    "        # ])),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\", PCA(n_components=10)),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            LogisticRegression(solver=\"saga\", max_iter=1000, random_state=42),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# TODO: Fit the pipeline on training data\n",
    "\n",
    "\n",
    "# TODO: Evaluate and print accuracy\n",
    "\n",
    "\n",
    "# TODO: Plot confusion matrix\n",
    "\n",
    "# Add as many code and markdown cells as needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ing3513",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
